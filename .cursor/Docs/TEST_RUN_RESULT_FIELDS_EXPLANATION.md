# Test Run Result Fields Explanation

## Overview

The `test_run_results` table contains several fields that are designed to capture detailed information about test case execution, especially for **automated test runs** or when tests fail. These fields are currently **not being populated** in the manual test execution workflow, which is why they appear empty.

## Field Purposes

### 1. `errorMessage` (error_message)
**Purpose**: Stores a human-readable error message when a test case fails.

**Intended Use**:
- For **automated tests**: Captured automatically when a test assertion fails
- For **manual tests**: Could be filled by the tester when marking a test as "Failed" or "Blocked"
- Example: "Login button not found", "Timeout waiting for element", "Assertion failed: expected 5 but got 3"

**Current Status**: ❌ **Empty** - Not being sent from frontend when updating test case status

**Why Empty**:
- The frontend `handleUpdateStatus` function only sends `status`, `executionTime`, and `executedAt`
- No UI input field exists for users to enter error messages
- Designed primarily for automated test frameworks that capture errors automatically

---

### 2. `stackTrace` (stack_trace)
**Purpose**: Stores the full stack trace when a test case fails, showing the exact code path that led to the failure.

**Intended Use**:
- For **automated tests**: Automatically captured from exceptions/errors thrown during test execution
- For **manual tests**: Typically not applicable (manual testers don't generate stack traces)
- Example: Full Java/Python/JavaScript stack trace showing file names, line numbers, and call stack

**Current Status**: ❌ **Empty** - Not being sent from frontend

**Why Empty**:
- Stack traces are generated by programming languages/runtimes, not by manual testers
- This field is primarily for automated test execution frameworks (Selenium, Cypress, Playwright, etc.)
- Manual test execution doesn't produce stack traces

---

### 3. `screenshots` (JSON field)
**Purpose**: Stores an array of screenshot URLs or base64-encoded images captured during test execution.

**Intended Use**:
- For **automated tests**: Automatically captured at failure points or key steps
- For **manual tests**: Could be uploaded by testers to document issues
- Format: `["url1", "url2"]` or `[{"url": "...", "timestamp": "..."}, ...]`

**Current Status**: ❌ **Empty** - Not being sent from frontend

**Why Empty**:
- Screenshots are currently uploaded via the **Comments** section (`TestRunAttachment` table), not directly to `test_run_results.screenshots`
- The `screenshots` field is designed for automated test frameworks that capture screenshots programmatically
- Manual testers use the comment/attachment feature instead

**Note**: There's a design decision here - should screenshots be:
- In `test_run_results.screenshots` (for automated tests) **OR**
- In `test_run_attachments` (for manual test comments) **OR**
- Both (different use cases)?

---

### 4. `logs` (logs)
**Purpose**: Stores execution logs from the test case run, including console output, debug information, and step-by-step execution details.

**Intended Use**:
- For **automated tests**: Captures console.log(), print statements, and framework logs
- For **manual tests**: Could be used for testers to document step-by-step execution notes
- Example: "Step 1: Navigate to login page ✓\nStep 2: Enter credentials ✓\nStep 3: Click login button ✗"

**Current Status**: ❌ **Empty** - Not being sent from frontend

**Why Empty**:
- Logs are typically generated by automated test frameworks
- Manual testers use the **Comments** section to document their findings instead
- No UI exists to input execution logs manually

---

## Current Architecture

### Manual Test Execution Flow (Current Implementation)
```
User clicks status button (Passed/Failed/Blocked/Skipped)
    ↓
Frontend: handleUpdateStatus()
    ↓
Sends to API: { status, executionTime, executedAt, executedBy }
    ↓
Backend: Updates test_run_results table
    ↓
Result: errorMessage, stackTrace, screenshots, logs remain NULL
```

### Automated Test Execution Flow (Intended but Not Implemented)
```
Automated test framework runs test case
    ↓
Test fails → Framework captures:
    - errorMessage: "Assertion failed"
    - stackTrace: Full exception stack trace
    - screenshots: Array of screenshot URLs
    - logs: Console output and execution logs
    ↓
API receives: { status: 'failed', errorMessage, stackTrace, screenshots, logs, ... }
    ↓
Backend: Stores all fields in test_run_results table
```

---

## Why These Fields Are Empty

1. **Manual Testing Workflow**: The current UI is designed for **manual test execution**, where testers click status buttons. These fields are designed for **automated test execution**.

2. **No UI Inputs**: There are no form fields in the frontend to capture:
   - Error messages
   - Stack traces (not applicable for manual)
   - Screenshots (using comments/attachments instead)
   - Execution logs

3. **Different Use Cases**:
   - **Manual tests**: Use Comments + Attachments for documentation
   - **Automated tests**: Use errorMessage, stackTrace, screenshots, logs for programmatic data

---

## Recommendations

### Option 1: Add UI for Manual Testers (Enhancement)
Add input fields in the test case detail view to allow manual testers to fill:
- **Error Message**: Text area when status is "Failed" or "Blocked"
- **Execution Logs**: Text area for step-by-step notes
- **Screenshots**: Already available via Comments/Attachments (could also link to `screenshots` field)

### Option 2: Keep Current Design (Recommended)
- Keep `errorMessage`, `stackTrace`, `screenshots`, `logs` for **automated test execution** (future feature)
- Manual testers continue using **Comments + Attachments** for documentation
- When automated test execution is implemented, these fields will be populated automatically

### Option 3: Hybrid Approach
- Allow manual testers to optionally fill `errorMessage` and `logs` when marking as "Failed"
- Keep `stackTrace` and `screenshots` for automated tests only
- Link `test_run_attachments` to `test_run_results.screenshots` for consistency

---

## Database Schema Reference

```sql
CREATE TABLE test_run_results (
  id BIGSERIAL PRIMARY KEY,
  test_run_id BIGINT NOT NULL,
  test_case_id BIGINT NOT NULL,
  status VARCHAR NOT NULL,
  execution_time INTEGER,           -- ✅ Used (duration in seconds)
  error_message TEXT,                -- ❌ Empty (for automated tests)
  stack_trace TEXT,                  -- ❌ Empty (for automated tests)
  screenshots JSONB,                 -- ❌ Empty (for automated tests)
  logs TEXT,                         -- ❌ Empty (for automated tests)
  executed_at TIMESTAMP,             -- ✅ Used (start time)
  executed_by BIGINT,                -- ✅ Used (assignee)
  ...
);
```

---

## Summary

| Field | Purpose | Why Empty | Solution |
|-------|--------|-----------|----------|
| `errorMessage` | Error message for failed tests | No UI input | Add text area for manual testers OR keep for automated only |
| `stackTrace` | Full stack trace from exceptions | Manual tests don't generate stack traces | Keep for automated tests only |
| `screenshots` | Screenshot URLs/JSON | Using comments/attachments instead | Link attachments OR keep separate |
| `logs` | Execution logs/console output | No UI input | Add text area OR keep for automated only |

**Current State**: These fields are **intentionally empty** for manual test execution. They are designed for **automated test execution frameworks** that will populate them programmatically.

